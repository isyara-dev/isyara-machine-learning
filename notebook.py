# -*- coding: utf-8 -*-
"""notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsP6__9NUfEkMvaHZE1DXxXXQyRpkBEW

# Machine Learning Project ISYARA: [BISINDO Dataset]


## Team Machine Learning ISYARA

- **Nama:** Rizka Alfadilla
- **Student ID:** MC299D5Y1776


- **Nama:** Davin Ghani Ananta Kusuma
- **Student ID:** MC299D5Y1599


- **Nama:** Nauval Gymnasti
- **Student ID:** MC299D5Y1716

## Packages/Library
"""

# Tunggu hingga semua library terinstall baru restart session

!pip uninstall -y mediapipe yfinance thinc grpcio-status ydf protobuf numpy

!pip install protobuf==5.29.0
!pip install numpy==1.24.4
!pip install mediapipe==0.10.21
!pip install yfinance ydf --upgrade --no-deps
!pip install grpcio-status==1.71.0
!pip install tensorflowjs

# Library Umum
import os
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from google.colab import files

# Machine Learning & Model Inference
import tensorflow as tf
import joblib
import mediapipe as mp

# Pelatihan dan evaluasi
import seaborn as sns
import tensorflowjs
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Mencetak versi TensorFlow yang sedang digunakan
print(f'Mediapipe Version: {mp.__version__}')
print(f'OpenCV Version: {cv2.__version__}')
print(f'Pandas Version: {pd.__version__}')
print(f'Numpy Version: {np.__version__}')
print(f'Scikit Learn Version: {sklearn.__version__}')
print(f'Tensorflow Version: {tf.__version__}')
print(f'Tensorflow JS Version: {tensorflowjs.__version__}')

# Menonaktifkan warning yang mungkin muncul
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Data Preparation

Dataset:
- https://www.kaggle.com/datasets/achmadnoer/alfabet-bisindo
- https://www.kaggle.com/datasets/yunitayupratiwi/bisindo-dataset/data
- https://www.kaggle.com/datasets/alfredolorentiars/bisindo-letter-dataset
- https://github.com/rhiosutoyo/Indonesian-Sign-Language-BISINDO-Hand-Sign-Detection-Dataset

### Data Loading
"""

from google.colab import drive
drive.mount('/content/drive')

# Path asal (di Drive)
source = 'drive/MyDrive/Colab/Capstone/BISINDO.zip'

# Path tujuan (lokal)
destination = 'BISINDO.zip'

# Copy file
shutil.copy(source, destination)

!unzip BISINDO.zip

folder_path = 'BISINDO'

# Membuat kamus yang menyimpan daftar gambar untuk setiap kelas
lung_image = {}

# Membaca semua nama file gambar untuk setiap kelas
for class_name in os.listdir(folder_path):
    class_path = os.path.join(folder_path, class_name)
    if os.path.isdir(class_path):
        lung_image[class_name] = os.listdir(class_path)

# Mengatur ukuran figure berdasarkan jumlah kelas dan jumlah gambar per baris
num_classes = len(lung_image.keys())
num_images = 5
fig, axs = plt.subplots(num_classes, num_images, figsize=(num_images * 4, num_classes * 4))
fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Menambahkan ruang antar subplot

for i, class_name in enumerate(lung_image.keys()):
    images = np.random.choice(lung_image[class_name], num_images, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(folder_path, class_name, image_name)
        try:
            img = Image.open(img_path)
            axs[i, j].imshow(img)
            axs[i, j].set_title(class_name, fontsize=12)
            axs[i, j].axis('off')
        except Exception as e:
            print(f"Gagal buka {img_path}: {e}")

plt.tight_layout()
plt.show()

def print_images_resolution(directory):
    unique_sizes = set()
    total_images = 0

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)
        image_files = os.listdir(subdir_path)
        num_images = len(image_files)
        print(f"{subdir}: {num_images}")
        total_images += num_images

        for img_file in image_files:
            img_path = os.path.join(subdir_path, img_file)
            with Image.open(img_path) as img:
                unique_sizes.add(img.size)

        for size in unique_sizes:
            print(f"- {size}")
        print("---------------")

    print(f"\nTotal: {total_images}")

folder_path = 'BISINDO'
print_images_resolution(folder_path)

# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data
file_name = []
labels = []
full_path = []

# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe
for path, subdirs, files in os.walk(folder_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path, 'file_name':file_name, "labels":labels})

# Plot distribusi gambar di setiap kelas
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

distribution_train['labels'].value_counts()

"""### Data Preprocessing"""

# Daftar label untuk satu lengan
one_hand_labels = ['C', 'E', 'I', 'J', 'L', 'O', 'R', 'U', 'V', 'Z']

# Pisahkan dataset berdasarkan jumlah lengan
one_hand_df = distribution_train[distribution_train['labels'].isin(one_hand_labels)]
two_hands_df = distribution_train[~distribution_train['labels'].isin(one_hand_labels)]

"""#### Extract Hand Landmark Features

##### YOU MUST RUN THIS
"""

# Inisialisasi MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Fungsi normalisasi landmark berdasarkan pergeseran dari pergelangan tangan (wrist)
def normalize_landmarks(landmarks, wrist):
    return [[lm[0] - wrist[0], lm[1] - wrist[1], lm[2] - wrist[2]] for lm in landmarks]

# Fungsi ekstraksi landmark dari gambar berdasarkan mode (one hand / two hands)
def extract_landmarks(image, is_one_hand):
    with mp_hands.Hands(static_image_mode=True, max_num_hands=2) as hands:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)

        if is_one_hand:
            if not results.multi_hand_landmarks:
                return None  # Tidak ada tangan terdeteksi

            # Ambil satu tangan dan normalisasi
            landmarks = [[lm.x, lm.y, lm.z] for lm in results.multi_hand_landmarks[0].landmark]
            wrist = landmarks[0]
            normalized = normalize_landmarks(landmarks, wrist)

            left_hand = [0.0] * 63  # 21 titik x 3 dimensi
            right_hand = [coord for lm in normalized for coord in lm]
            return left_hand + right_hand

        else:  # Mode dua tangan
            if not results.multi_hand_landmarks or len(results.multi_hand_landmarks) != 2:
                return None  # Harus ada 2 tangan

            left_hand = None
            right_hand = None

            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
                hand_label = results.multi_handedness[idx].classification[0].label
                landmarks = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]
                wrist = landmarks[0]
                normalized = normalize_landmarks(landmarks, wrist)

                if hand_label == "Left":
                    left_hand = [coord for lm in normalized for coord in lm]
                elif hand_label == "Right":
                    right_hand = [coord for lm in normalized for coord in lm]

            if left_hand is None or right_hand is None:
                return None  # Harus dua tangan dengan label jelas

            return left_hand + right_hand

"""##### Extrack Landmark from image to CSV"""

# Fungsi utama untuk memproses gambar dari DataFrame dan menyimpan ke CSV
def process_images_from_df(df, csv_filename, is_one_hand=False):
    data = []

    for idx, row in df.iterrows():
        label = row['labels']
        img_path = row['path']
        img = cv2.imread(img_path)

        if img is None:
            print(f'Gagal membaca gambar: {img_path}')
            continue

        landmarks = extract_landmarks(img, is_one_hand)

        if landmarks is not None:
            data.append([label, img_path] + landmarks)

    # Kolom nama fitur
    columns = ['label', 'path'] + \
              [f'left_{i}_{axis}' for i in range(21) for axis in ['x', 'y', 'z']] + \
              [f'right_{i}_{axis}' for i in range(21) for axis in ['x', 'y', 'z']]

    df_out = pd.DataFrame(data, columns=columns)
    df_out.to_csv(csv_filename, index=False)
    print(f'Fitur landmark disimpan di {csv_filename}')

process_images_from_df(one_hand_df, "landmarks_one_hand.csv", is_one_hand=True)
process_images_from_df(two_hands_df, "landmarks_two_hand.csv", is_one_hand=False)

# Baca kedua CSV
df_one_hand = pd.read_csv("landmarks_one_hand.csv")
df_two_hand = pd.read_csv("landmarks_two_hand.csv")

# Gabungkan
df_combined = pd.concat([df_one_hand, df_two_hand], ignore_index=True)

# Urutkan berdasarkan label
df_combined_sorted = df_combined.sort_values(by="label").reset_index(drop=True)

# Simpan ke file baru
df_combined_sorted.to_csv("landmarks_combined_sorted.csv", index=False)

print("Gabungan CSV selesai dan disimpan sebagai landmarks_combined_sorted.csv")

df = pd.read_csv("landmarks_combined_sorted.csv")

df.head(10)

df['label'].value_counts()

"""#### Oversampling Data"""

max_count = df_combined_sorted['label'].value_counts().max()

# Oversampling manual
oversampled_df = (
    df_combined_sorted.groupby('label')
    .apply(lambda x: x.sample(max_count, replace=True, random_state=42))
    .reset_index(drop=True)
)

print(oversampled_df['label'].value_counts())
oversampled_df.to_csv("landmarks_oversampled.csv", index=False)

"""#### Split Dataset"""

def split_train_test(csv_filename, train_file, test_file, train_size=0.8):
    # Load dataset
    df = pd.read_csv(csv_filename)
    df = df.drop(columns=['path'])

    # Pisahkan fitur dan label
    X = df.drop(columns=['label'])
    y = df['label']

    # Split dataset menjadi train dan test
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, stratify=y, random_state=42)

    # Buat DataFrame untuk masing-masing set
    train_df = pd.concat([y_train, X_train], axis=1)
    test_df = pd.concat([y_test, X_test], axis=1)

    # Simpan ke file CSV
    train_df.to_csv(train_file, index=False)
    test_df.to_csv(test_file, index=False)

    print(f"Dataset dibagi menjadi:")
    print(f"Train Set: {train_df.shape[0]} sampel")
    print(f"Test Set: {test_df.shape[0]} sampel")

split_train_test('landmarks_oversampled.csv', 'train.csv', 'test.csv')

"""## Modelling"""

# Parameter
input_dim = 126  # 42 x 3 (x, y, z) untuk Left dan Right
epochs = 100
batch_size = 32
checkpoint_dir = './checkpoints'
saved_model_cnn_dir = './saved_models_cnn'
saved_model_dnn_dir = './saved_models_dnn'

# Pastikan folder ada
os.makedirs(checkpoint_dir, exist_ok=True)
os.makedirs(saved_model_cnn_dir, exist_ok=True)
os.makedirs(saved_model_dnn_dir, exist_ok=True)

# Load Dataset
def load_dataset(train_file, test_file):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)

    X_train = train_df.drop(columns=['label']).values
    y_train = train_df['label'].values

    X_test = test_df.drop(columns=['label']).values
    y_test = test_df['label'].values

    # Encode labels
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    return X_train, y_train, X_test, y_test, label_encoder

# Load data
X_train, y_train, X_test, y_test, label_encoder = load_dataset('train.csv', 'test.csv')

# Simpan LabelEncoder
joblib.dump(label_encoder, "label_encoder.pkl")
print("Label encoder berhasil dibuat dan disimpan.")

# Reshape untuk CNN 1D
X_train_cnn = X_train.reshape((X_train.shape[0], input_dim, 1))
X_test_cnn = X_test.reshape((X_test.shape[0], input_dim, 1))

num_classes = len(label_encoder.classes_)

# --- CNN 1D Model ---
def create_cnn_model(input_shape, num_classes):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# --- DNN Model ---
def create_dnn_model(input_dim, num_classes):
    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(input_dim,)),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# --- Callback untuk CNN ---
cnn_callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    tf.keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'best_model_cnn.h5'),
        monitor='val_accuracy', save_best_only=True, verbose=1
    ),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    tf.keras.callbacks.TensorBoard(log_dir='./logs/cnn')
]

# --- Callback untuk DNN ---
dnn_callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    tf.keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'best_model_dnn.h5'),
        monitor='val_accuracy', save_best_only=True, verbose=1
    ),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    tf.keras.callbacks.TensorBoard(log_dir='./logs/dnn')
]

# --- Training CNN 1D ---
cnn_model = create_cnn_model((input_dim, 1), num_classes)
print("\nTraining CNN 1D...")
cnn_history = cnn_model.fit(
    X_train_cnn, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_test_cnn, y_test),
    callbacks=cnn_callbacks
)

# --- Training DNN ---
dnn_model = create_dnn_model(input_dim, num_classes)
print("\nTraining DNN...")
dnn_history = dnn_model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_test, y_test),
    callbacks=dnn_callbacks
)

"""## Evaluation and Visualization"""

print("\nEvaluating CNN 1D Model:")
cnn_loss, cnn_acc = cnn_model.evaluate(X_test_cnn, y_test)
cnn_train_acc = cnn_history.history['accuracy'][-1]
print(f"Train Accuracy (CNN 1D): {cnn_train_acc:.4f}")
print(f"Test Accuracy (CNN 1D): {cnn_acc:.4f}")

# Classification Report
y_pred_cnn = cnn_model.predict(X_test_cnn).argmax(axis=1)
print("\nClassification Report (CNN 1D):")
print(classification_report(y_test, y_pred_cnn, target_names=label_encoder.classes_))

print("\nEvaluating DNN Model:")
dnn_loss, dnn_acc = dnn_model.evaluate(X_test, y_test)
dnn_train_acc = dnn_history.history['accuracy'][-1]
print(f"Train Accuracy (DNN): {dnn_train_acc:.4f}")
print(f"Test Accuracy (DNN): {dnn_acc:.4f}")

# Classification Report
y_pred_dnn = dnn_model.predict(X_test).argmax(axis=1)
print("\nClassification Report (DNN):")
print(classification_report(y_test, y_pred_dnn, target_names=label_encoder.classes_))

# --- Confusion Matrix (CNN 1D) ---
cm_cnn = confusion_matrix(y_test, y_pred_cnn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix - CNN 1D')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# --- Confusion Matrix (DNN) ---
cm_dnn = confusion_matrix(y_test, y_pred_dnn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dnn, annot=True, fmt='d', cmap='Greens',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix - DNN')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# --- Plot Akurasi dan Loss CNN ---
acc = cnn_history.history['accuracy']
val_acc = cnn_history.history['val_accuracy']
loss = cnn_history.history['loss']
val_loss = cnn_history.history['val_loss']
epochs_range = range(len(acc))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, 'r', label='Train Acc')
plt.plot(epochs_range, val_acc, 'b', label='Val Acc')
plt.title('CNN: Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, 'r', label='Train Loss')
plt.plot(epochs_range, val_loss, 'b', label='Val Loss')
plt.title('CNN: Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# --- Plot Akurasi dan Loss DNN ---
acc = dnn_history.history['accuracy']
val_acc = dnn_history.history['val_accuracy']
loss = dnn_history.history['loss']
val_loss = dnn_history.history['val_loss']
epochs_range = range(len(acc))

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, 'r', label='Train Acc')
plt.plot(epochs_range, val_acc, 'b', label='Val Acc')
plt.title('DNN: Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, 'r', label='Train Loss')
plt.plot(epochs_range, val_loss, 'b', label='Val Loss')
plt.title('DNN: Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""## Model Conversion

### SavedModel (TensorFlow)
"""

tf.saved_model.save(cnn_model, saved_model_cnn_dir)

shutil.make_archive('saved_models_cnn', 'zip', 'saved_models_cnn')
from google.colab import files
files.download('saved_models_cnn.zip')

tf.saved_model.save(dnn_model, saved_model_dnn_dir)

shutil.make_archive('saved_models_dnn', 'zip', 'saved_models_dnn')
from google.colab import files
files.download('saved_models_cnn.zip')

"""### TensorFlow.js (TFJS)"""

!tensorflowjs_converter \
    --input_format=tf_saved_model \
    /content/saved_models_cnn/ \
    /content/cnn_tfjs_model

shutil.make_archive('cnn_tfjs_model', 'zip', 'cnn_tfjs_model')
files.download('cnn_tfjs_model.zip')

!tensorflowjs_converter \
    --input_format=tf_saved_model \
    /content/saved_models_dnn/ \
    /content/dnn_tfjs_model

shutil.make_archive('dnn_tfjs_model', 'zip', 'dnn_tfjs_model')
files.download('dnn_tfjs_model.zip')

"""## Inference

### How to Run Inference

---

### 1. Persiapan File dan Folder

* Pastikan file berikut ada di folder kerja Google Colab:

  * `label_encoder.pkl` — file label encoder hasil training.
  * Folder model CNN (`saved_models_cnn/`) yang berisi model TensorFlow SavedModel untuk CNN.
  * Folder model DNN (`saved_models_dnn/`) yang berisi model TensorFlow SavedModel untuk DNN.

---

### 2. Pastikan LIBRARY dan FUNGSI  ada

* Jalankan sel `Packages/Library` untuk install semua package dan library yang diperlukan.
* Jalankan sel `YOU MUST RUN THIS` untuk ekraksi landmark.

---

### 3. Install Dependencies

* Jalankan sel `Script Inference` secara berurutan.
* Pilih gambar yang berisi 1 atau 2 tangan yang ingin kamu inferensikan.
* Tunggu hasil prediksi muncul di output.

---

### 4. Tips & Troubleshooting

* Jika muncul error file tidak ditemukan, cek kembali apakah file model dan label encoder sudah diupload dan path sudah benar.
* Jika landmark tidak terdeteksi, coba gunakan gambar tangan dengan posisi dan pencahayaan lebih baik.
* Pastikan fungsi `extract_landmarks()` sesuai output yang diharapkan.

### Script Inference
"""

# Load encoder dan model
label_encoder = joblib.load('label_encoder.pkl')
cnn_model = tf.keras.layers.TFSMLayer('./saved_models_cnn', call_endpoint='serving_default')
dnn_model = tf.keras.layers.TFSMLayer('./saved_models_dnn', call_endpoint='serving_default')

# Fungsi untuk menampilkan gambar
def display_image(image):
    print()
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(8,6))
    plt.imshow(image_rgb)
    plt.axis('off')
    plt.title("Gambar yang Diupload")
    plt.show()

# Fungsi untuk prediksi gesture berdasarkan landmark vector
def predict_gesture(landmark_vector):
    X_flat = np.array(landmark_vector).reshape(1, -1)              # Untuk DNN
    X_seq = X_flat.reshape(1, 126, 1).astype(np.float32)           # Untuk CNN

    # ===== CNN Prediction =====
    pred_cnn = cnn_model(X_seq)
    cnn_probs = list(pred_cnn.values())[0].numpy()[0]
    top3_cnn_idx = np.argsort(cnn_probs)[::-1][:3]  # 3 indeks tertinggi
    top3_cnn = [
        (label_encoder.inverse_transform([i])[0], cnn_probs[i] * 100)
        for i in top3_cnn_idx
    ]

    # ===== DNN Prediction =====
    pred_dnn = dnn_model(X_flat.astype(np.float32))
    dnn_probs = list(pred_dnn.values())[0].numpy()[0]
    top3_dnn_idx = np.argsort(dnn_probs)[::-1][:3]
    top3_dnn = [
        (label_encoder.inverse_transform([i])[0], dnn_probs[i] * 100)
        for i in top3_dnn_idx
    ]

    return top3_cnn, top3_dnn

# Fungsi utama
def main():
    # Upload gambar
    uploaded = files.upload()
    if not uploaded:
        print("❌ Tidak ada gambar diupload.")
        return

    file_name = list(uploaded.keys())[0]
    image = cv2.imread(file_name)
    display_image(image)

    # Gunakan MediaPipe Hands untuk mengetahui jumlah tangan
    with mp_hands.Hands(static_image_mode=True, max_num_hands=2) as hands:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)
        num_hands = len(results.multi_hand_landmarks) if results.multi_hand_landmarks else 0

    print(f"\n🖐️ Jumlah tangan terdeteksi: {num_hands}")

    # Ekstraksi landmark menggunakan fungsi kamu
    if num_hands == 1:
        landmark_vector = extract_landmarks(image, is_one_hand=True)
    elif num_hands == 2:
        landmark_vector = extract_landmarks(image, is_one_hand=False)
    else:
        landmark_vector = None

    if landmark_vector:
        # Tampilkan koordinat normalisasi
        coords = np.array(landmark_vector).reshape(-1, 3)
        print("\n📍 Koordinat Landmark (Ternormalisasi):\n")
        for i in range(21):
            print(f"left_{i}_x = {coords[i][0]:.4f}, left_{i}_y = {coords[i][1]:.4f}, left_{i}_z = {coords[i][2]:.4f}")
        for i in range(21, 42):
            j = i - 21
            print(f"right_{j}_x = {coords[i][0]:.4f}, right_{j}_y = {coords[i][1]:.4f}, right_{j}_z = {coords[i][2]:.4f}")

        top3_cnn, top3_dnn = predict_gesture(landmark_vector)

        print("\n🧠 Hasil Prediksi Gesture:")
        print("\n🔹 CNN Model:")
        for label, conf in top3_cnn:
            print(f"   - {label} ({conf:.2f}%)")

        print("\n🔹 DNN Model:")
        for label, conf in top3_dnn:
            print(f"   - {label} ({conf:.2f}%)")

    else:
        print("\n❌ Tidak dapat mengekstrak landmark tangan.")
        print("📌 Pastikan kondisi gambar memenuhi syarat berikut:")
        print("   - Tangan terlihat jelas dan tidak buram.")
        print("   - Pencahayaan cukup (tidak terlalu gelap atau terang).")
        print("   - Seluruh tangan (minimal satu atau dua) masuk dalam frame.")
        print("   - Latar belakang tidak terlalu ramai sehingga tangan mudah terdeteksi.")
        print("📝 Coba unggah ulang gambar dengan posisi tangan yang lebih jelas.")

# Jalankan fungsi utama
main()

!pip freeze > requirements.txt